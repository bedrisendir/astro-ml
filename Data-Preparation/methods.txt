DATA PREP METHODS
-Check for NaN inputs. Copy from previous data point if available. Each data point ( 15 sec )

-Pre-define threshold for each column. (very small number)

-Compute de standard deviation of each column. Compare results with thresholds. Remove columns if they are below predefined thresholds.

-Feature Reduction PCA.
Find highly co-related columns. 
We should expect something like this after running PCA.
PCA also chooses the best combination from a set of possible linear functions that
retains maximu m variance of our data. In our case,the initial 163 features were reduced to around 50 features after PCA.

2 steps:
1- Prediction. Traning data --> history of Cluster incidents (ganglia)
  Then feed the model for real-time testing. 
  The testing and training data have similar steps as follows:
    a. Data Collection
	system-level and Hadoop metrics (we have)
	In the paper, they have 163 metrics with at minute intervals (15 sec. in ours)
	They choose one-hour data before a problem is detected as abnormal training dataset.
	So traning dataset has anomalous behavior. Then they choose five-hour data after the node was restored as normal data
	(We dont have any anomalous behavior in our dataset....)
    b. Data Preprocessing
	Dataset should be tabular format (column and rows with timestamp, we have)
	The technique that they use doesn't allow NaN data, so they copy data from the last available time
	For some metrics, data for the whole five-hour period of time are empty. They fill out those empty data with 
	  a constant value, so that it is ignored at a later time during training phase.
	
	Compute standard deviation for each column. If the standard deviation is less than a pre-defined threshold,
	  generally a very small number, we remove that column as input to the computation for our model.
	After removing these low-variance columns, they apply Principal Component Analysis(PCA) for future reduction 
	(http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)
	(http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)

	For example, hadoop.app.cpuUser and hadoop.app.cpuNice are highly correlated, Appyling PCA, they can find linear
	  combination of correlated features. PCA also chooses the best combination from a set of possible linear functions
	  that retains maximum variance of our data. So 163 features reduced to 50 features after PCA

    c. Multivarite Gaussian Distribution
	For building a model by using training data set.
	Calculate mean and covariance of training data set so that they can compute probablity using 
	  a multivariate Guassian Function (numpy.random.multivariate_normal or scipy.stats.multivariate_normal)
	(http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multivariate_normal.html)
	takes mean and covariance as an argument we need to predefine it.
	
	Gaussian Distributed Function
	(https://docs.python.org/2/library/random.html) Generate random numbers
	(http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) random samples from a normal(Gaussian)
	  distribution

    d. Cut-off threshold to differentiate normal and abnormal
	We want to find out which data nodes fall within an abnormal region from Gaussian Distribution Function, so that
	  we can mark them as anomalous nodes. In the system, the probability for a data point can be very small, even for
	  a normal data node, and potentially be classified as abnormal, which may also indicate false-positives. They seek to
	  find a THRESHOLD to reduce the false-positives even more and also maximize true anomalies or true-positives.
	
	d.1. Matthews Correlation Coefficient (MCC)
	  We need to find True-positives and negatives and also false-positives and negatives in each column cause 
	    formula needs them
	  (http://blog.nextgenetics.net/?e=47)
	  (http://en.wikipedia.org/wiki/Matthews_correlation_coefficient)
	
     	d.2. Half total error rate (AAcc)

    e. Anomaly score calculation
	Function to calculate anomaly score between 0 and 10. 10 is the highest anomaly node.
	They pick two threshold, a soft and hard. The thresholds are determined from their training dataset. How???

    f. Determining features that contributes to anomaly
	Single-variate Gaussian Distribution to deduce (sonuc cikarmak) this info (after detect anomaly node)
	Compute probability value for each of the feauteres seperately and factorize them to determine the final probability.
	As the data node is anomalous, the factorized value should be very small and there must be features with significantly
	  smaller individual probabilities than others, and those contribute to lower the final probability.
	We can select these features with lower individual probabilities and sort them in ascending order.
	Find the five top five features provide a good indication of the root cause for the anomaly, and call them "Influencers"
	Once we calculate the anomaly score and get the list of influencers as descried above, we pass this information to the
	  scheduler!!..
	  
	  
2- Scheduling
	
		
--------
Link for anomaly detection
https://github.com/twitter/AnomalyDetection/blob/master/R/ts_anom_detection.R
